# Import necessary libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from textblob import TextBlob
import requests
from bs4 import BeautifulSoup
import tweepy
import nltk
from nltk.corpus import stopwords

# Data Collection (e.g., Twitter)
def collect_tweets(api_key, api_secret_key, access_token, access_token_secret, search_term, count=100):
    # Setup Twitter API
    auth = tweepy.OAuthHandler(api_key, api_secret_key)
    auth.set_access_token(access_token, access_token_secret)
    api = tweepy.API(auth)
    
    # Collect tweets
    tweets = tweepy.Cursor(api.search, q=search_term, lang="en").items(count)
    tweet_list = [[tweet.text, tweet.created_at] for tweet in tweets]
    df = pd.DataFrame(tweet_list, columns=['Text', 'Timestamp'])
    return df

# Data Cleaning
def clean_text(text):
    text = text.lower()
    text = re.sub(r'http\S+', '', text)  # Remove URLs
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # Remove special characters
    text = ' '.join([word for word in text.split() if word not in stopwords.words('english')])
    return text

# Sentiment Analysis
def analyze_sentiment(text):
    analysis = TextBlob(text)
    return analysis.sentiment.polarity

# Example usage
if __name__ == "__main__":
    # Set Twitter API credentials (use your own keys)
    API_KEY = 'your_api_key'
    API_SECRET = 'your_api_secret_key'
    ACCESS_TOKEN = 'your_access_token'
    ACCESS_SECRET = 'your_access_secret_token'

    # Collect and clean tweets
    tweets_df = collect_tweets(API_KEY, API_SECRET, ACCESS_TOKEN, ACCESS_SECRET, 'product_name', 100)
    tweets_df['Cleaned_Text'] = tweets_df['Text'].apply(clean_text)
    tweets_df['Sentiment'] = tweets_df['Cleaned_Text'].apply(analyze_sentiment)
    
    # Plot sentiment distribution
    sns.histplot(tweets_df['Sentiment'])
    plt.title('Sentiment Analysis of Product Reviews')
    plt.show()
